#General file 

'''
#Software
GATK version 4.0.5.2
Platypus version 0.8.1
samtools version 1.7, htslib version 1.7.2
freebayes v1.2.0
vcftools
Baysic v1.0
bedtools v2.26.0
'''

#Config directions:
BAM_DIR="../temp/alignments/Mphar"
SPECIES = "Mphar"
NLINE = 193779
SPECIES_SNPEFF="mphar"
REF_PREFIX = "../ref/GCF_000980195.1_M.pharaonis_V2.0_genomic"
RMINDIV = "GEN0051Aligned.sortedByCoord.out"

#BAM_DIR = "../temp/alignments/Amel"
#SPECIES = "Amel"
#NLINE = 232191 #Get this from "wc -l Amel.exons.bed"...ideally would compute this automatically
#SPECIES_SNPEFF = "a_mellifera"
#REF_PREFIX = "../ref/GCF_000002195.4_Amel_4.5_genomic"
#RMINDIV = "SRR957079"

SAMPLES, = glob_wildcards(BAM_DIR+"/{sample}.bam")
PICARD = "/home/warnerm/local_install/picard.jar"

GFF = REF_PREFIX+".gff"
REF = REF_PREFIX+".fna"
THREADS = 20
PLATYPUS = "/home/warnerm/local_install/Platypus_0.8.1/Platypus.py"
CALLER=["freebayes","samtools","GATK"]
STAGE = ["larva","pupa","head","thorax","abdomen"]
ALL_SPEC= ["Mphar","Amel"]


rule all:
	input: "../out/"+SPECIES+".substitutions.csv","../out/"+SPECIES+".gene.pi.csv",
		"../out/"+SPECIES+".alpha.csv",
		"../out/MKtest_globalAlpha_locusF_"+SPECIES,
		"../out/"+SPECIES+".snipre_results.csv",
		"../temp/aln_read/"+SPECIES+".folded.saf.pos.gz"


rule makeBED:
	input: GFF
	output: "../ref/{SPECIES}.bed"
	shell: """gff2bed {input} > {output}"""

rule makeDict:
	input: REF
	output: REF_PREFIX+".dict"
	shell: "java -jar {PICARD} CreateSequenceDictionary R={input} O={output}"

#Sorts the bam files; necessary for at least samtools
rule PicardGroups:
	input: BAM_DIR+"/{sample}.bam"
	output: "../temp/aln_read/{sample}.bam"
	shell: """java -jar {PICARD} AddOrReplaceReadGroups I={input} O={output} SORT_ORDER=coordinate \
				CREATE_INDEX=true RGPL=illumina RGID={wildcards.sample} RGSM={wildcards.sample} RGLB=lib1 RGPU=unit1"""


#Split up targets to run freebayes and gatk in parallel
rule subset_exons:
	input: "../ref/{SPECIES}.bed"
	output: expand("../ref/{{SPECIES}}.sub_{i}.bed",i=range(20))
	run:
		total = int(NLINE)
		inc = int(total/20)+1
		for i in range(20):
			if i == 19:
				numL = total
				t = total - inc*19
			else:
				numL = inc*(i+1)
				t = inc
			f = output[i]
			shell("head -n {numL} {input} | tail -n {inc} > {f}")

#'-=' means report genome qualities, the params makes a list of input files, min-alternate-count sets minimum number of chromosomes sampled to consider an allele
#Using target= exons.bed because we only want to call variants for exons
rule freeBayes:
	input: expand("../temp/aln_read/{sample}.bam",sample=SAMPLES),bed="../ref/"+SPECIES+".sub_{i}.bed"
	output: "../var/"+SPECIES+".fb_{i}.vcf"
	params: 
		files = "-b " + " -b ".join(expand("../temp/aln_read/{sample}.bam",sample=SAMPLES)),
	shell: "freebayes -= {params.files} -v {output} -f {REF} --targets {input[bed]}"

rule mergeFB:
	input: expand("../var/"+SPECIES+".fb_{i}.vcf",i=range(20))
	output: "../var/"+SPECIES+".freebayes.vcf"
	shell: "vcf-concat {input} > {output}"

#Just get first three columns of bed files
rule editBed:
	input: "../ref/"+SPECIES+".sub_{i}.bed"
	output: "../ref/"+SPECIES+".sub_{i}_edit.bed"
	shell: """cat {input} | awk '{{print$1"\t"$2"\t"$3}}' > {output}"""

#Convert bed files to interval lists for gatk
rule intervalList:
	input: bed="../ref/"+SPECIES+".sub_{i}_edit.bed",dict=rules.makeDict.output
	output: "../ref/"+SPECIES+".sub_{i}.interval_list"
	shell: """export PATH=~/local_install/jdk1.8.0_05/bin:$PATH; java -jar {PICARD} BedToIntervalList I={input[bed]} O={output} SD={input[dict]}"""

rule GATK:
	input: expand("../temp/aln_read/{sample}.bam",sample=SAMPLES),intervals= "../ref/"+SPECIES+".sub_{i}.interval_list",dict=rules.makeDict.output
	output: "../var/"+SPECIES+".gatk_{i}.vcf"
	params: files = "-I " + " -I ".join(expand("../temp/aln_read/{sample}.bam",sample=SAMPLES))
	shell: """export PATH=~/local_install/jdk1.8.0_05/bin:$PATH; gatk --java-options "-Xmx30g" HaplotypeCaller \
				-R {REF} \
				{params.files} \
				-O {output} \
				-L {input[intervals]} \
				--heterozygosity 0.002 \
				--mbq 20 \
				--max-alternate-alleles 2"""

rule mergeGATK:
	input: expand("../var/"+SPECIES+".gatk_{i}.vcf",i=range(20))
	output: "../var/"+SPECIES+".GATK.vcf"
	shell: "vcf-concat {input} > {output}"

rule samtools:
	input: BAMs = expand("../temp/aln_read/{sample}.bam",sample=SAMPLES),bed = "../ref/"+SPECIES+".exons.bed"
	output: "../var/"+SPECIES+".samtools.vcf"
	shell: "samtools mpileup -l {input[bed]} -ugf {REF} {input[BAMs]} | bcftools call -vc - | vcfutils.pl varFilter -D 500 > {output}"


#remove insertions and deletions
rule rmIndel:
	input: "../var/"+SPECIES+".{VCFcaller}.vcf"
	output: "../var/"+SPECIES+".{VCFcaller}.snp.recode.vcf"
	shell: "vcftools --vcf {input} --remove-indels --out ../var/{SPECIES}.{wildcards.VCFcaller}.snp --recode --recode-INFO-all"

rule allelicPrimitives:
	input: "../var/"+SPECIES+".{VCFcaller}.snp.recode.vcf"
	output: "../var/"+SPECIES+".{VCFcaller}.snp.primatives.vcf"
	shell: "cat {input} | vcfallelicprimitives -kg > {output}"

rule vcfSort:
	input: "../var/"+SPECIES+".{VCFcaller}.snp.primatives.vcf"
	output: "../var/"+SPECIES+".{VCFcaller}.snp.primatives.sorted.vcf"
	shell: "vcf-sort {input} > {output}"

# generate consensus SNP calls
rule BAYSIC: 	
	input: expand("../var/"+SPECIES+".{VCFcaller}.snp.primatives.sorted.vcf", VCFcaller=CALLER)
	output: "../var/"+SPECIES+".consensus.vcf.pos","../var/"+SPECIES+".consensus.vcf"
	run: 
		infiles = "".join([" --vcf " + i for i in input])
		shell("perl baysic.pl --statsOutFile ../var/combined.stats --pvalCutoff 0.8 {} --countsOutFile ../var/combined.cts --vcfOutFile {{output[1]}}".format(infiles))

# select bi-allelic consensus sites
rule consensusFilter:
    input: "../var/"+SPECIES+".samtools.snp.primatives.sorted.vcf","../var/"+SPECIES+".consensus.vcf.pos"
    output: "../var/"+SPECIES+".final.recode.vcf"
    shell: "vcftools --vcf {input[0]} --positions {input[1]} --max-alleles 2 --remove-indels --recode --out  ../var/{SPECIES}.final"

# estimate SNP effects
rule snpEff:
	input: rules.consensusFilter.output
	output: "../var/"+SPECIES+".snpEff.txt"
	shell: "java -Xmx7g -jar /home/warnerm/local_install/snpEff/snpEff.jar -no-utr -no-upstream -no-intron -no-intergenic -no-downstream {SPECIES_SNPEFF} {input} >  {output}"

# determine which SNPs are fixed and which are polymorphic
# for this we remove the outgroup and compute frequencies
#Apis cerana is SRR957079
#NOTE: for some reason there are duplicate lines in the output for this rule
rule fixedPolymorphic:	
	input: rules.consensusFilter.output
	output: "../var/"+SPECIES+".snps.csv"
	shell: """vcftools --vcf {input} --remove-indv {RMINDIV} --freq; \
		awk -v OFS="," ' NR>1 {{split($5,a,":"); if((a[2]=="1") || (a[2]=="0")) state="F"; else state="P"; print $1,$2,state}}' out.frq > {output} """

rule getCDS:
	input: GFF, REF
	output: "../ref/"+SPECIES+".cds.fa"
	shell: "gffread {input[0]} -g {input[1]} -x {output}"

rule filterLongest:
	input: rules.getCDS.output
	output: "../ref/"+SPECIES+".longest.fa"
	shell: "python filter_longest.py {input} > {output}"

rule getLengths:
	input: rules.filterLongest.output
	output: "../out/"+SPECIES+"_lengths.txt"
	shell: "python2.7 get_lengths.py {input} {output}"

# exports silent and replacement sites from snpEff
rule parseSilentReplacement:
	input: rules.filterLongest.output, rules.snpEff.output
	output: "../var/"+SPECIES+".annotation.csv"
	shell: "python2.7 parse_silentReplacement.py {input} > {output}"

# calculate how many synonymous vs_non-synonymous changes are possible
rule silentReplacement:
	input: rules.filterLongest.output
	output: "../var/"+SPECIES+".silentReplacement.csv"
	shell: "python2.7 silent_replacement.py {input} > {output}"

#Tabulate substitutions
rule tab_subs:
	input: rules.parseSilentReplacement.output,rules.fixedPolymorphic.output,rules.silentReplacement.output
	output: "../out/"+SPECIES+".substitutions.csv"
	shell: "Rscript --vanilla tabulate_substitutions.R {input} {output}"


#Make MKinput files for alpha classes (both species)
rule MKclass_input:
	input: "../out/"+SPECIES+".substitutions.csv","../data/DEtests.RData"
	output: expand("../MK_alpha_input/{stage}.{species}.csv",stage=STAGE,species=SPECIES)
	shell: "Rscript MKalphaInput.R {input}"

#Calculate constraint from MKtest
rule MKtest_constraint:
	input: "../MK_alpha_input/abdomen.{species}.csv"
	output: "../out/MKtest_globalAlpha_locusF_{species}"
	shell: "MKtest -a 1 -f 2 -o {output} {input}"

#Bootstrap MKtest for alpha results (class-specific alpha)
rule MKtest_alpha:
	input: "../MK_alpha_input/{stage}.{species}.csv"
	output: "../out/alpha_locusF_{stage}.{species}.csv"
	shell: "MKtest -a 3 -f 2 -P -200 -o {output} {input}"

#Collect alpha results into one file
rule collectAlpha:
	input: expand("../out/alpha_locusF_{stage}.{species}.csv",stage=STAGE,species=SPECIES)
	output: "../out/"+SPECIES+".alpha.csv"
	shell: "Rscript combineAlpha.R ../out/alpha_locusF_ {SPECIES} {output}"

#Calculate pi within honeybee population
rule calcPi:
	input: rules.consensusFilter.output
	output: "../out/"+SPECIES+".sites.pi"
	shell: "vcftools --vcf {input} --site-pi --remove-indv {RMINDIV} --out ../out/{SPECIES}"

#Calculate gene-wise pi
rule genePi:
	input: rules.parseSilentReplacement.output,rules.calcPi.output
	output: "../out/"+SPECIES+".gene.pi.csv"
	shell: "Rscript --vanilla pi_per_gene.R {input} {output}"

#Run snipre to get gamma
rule snipre:
	input: rules.silentReplacement.output, rules.parseSilentReplacement.output,rules.fixedPolymorphic.output
	output: "../out/"+SPECIES+".snipre_results.csv"
	shell: "Rscript --vanilla snipre.R {input} {output}"

rule foldedSFS:
	input: "../temp/aln_read/files"+SPECIES #List of BAM files
	output: "../temp/aln_read/"+SPECIES+".folded.saf.pos.gz"
	shell: "cd ../temp/aln_read/; angsd -bam files{SPECIES} -doSaf 1 -out {SPECIES}.folded -anc ../{REF} -GL 2 -fold 1 -P {THREADS}"



